{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f84dc395-046e-40ed-b6b3-40774e12761d",
   "metadata": {},
   "source": [
    "# 0. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c83499ac-f736-46fd-9898-e6125ce7e9b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models.vgg import vgg16\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# paper reference: \"https://arxiv.org/abs/1906.01493\"\n",
    "# calculation reference: \"https://www.baeldung.com/cs/pca\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b793845d-d90e-450d-ba24-d30460e296e1",
   "metadata": {},
   "source": [
    "# 1. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "881e0808-7ab6-48df-91fc-e9f0dc19ce3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_PCA(feature, threshold = 0.95, status_print = False):\n",
    "\n",
    "    total_channel = feature.shape[1]\n",
    "\n",
    "    activations = (feature.data).cpu().numpy()\n",
    "    # print('shape of activations are:',activations.shape)\n",
    "    a=activations.swapaxes(1,2).swapaxes(2,3)\n",
    "    a_shape=a.shape\n",
    "    # print('reshaped ativations are of shape',a.shape)\n",
    "    # raw_input()\n",
    "\n",
    "    pca = PCA() #number of components should be equal to the number of filters\n",
    "    pca.fit(a.reshape(a_shape[0]*a_shape[1]*a_shape[2],a_shape[3]))\n",
    "    a_trans=pca.transform(a.reshape(a_shape[0]*a_shape[1]*a_shape[2],a_shape[3]))\n",
    "    # print('explained variance ratio is:',pca.explained_variance_ratio_)\n",
    "    # raw_input()\n",
    "    cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "    d = np.argmax(cumsum >= threshold)\n",
    "    \n",
    "    # print(cumsum.shape)\n",
    "    # print(\"pca: \", pca.components_.shape)\n",
    "    \n",
    "    # importance_ratio = d/total_channel\n",
    "\n",
    "    if status_print:\n",
    "        print('need at least {} filter(s) out of {} components to exceed threshold'.format(d, total_channel))\n",
    "    \n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f60471-6454-4a1a-b047-57cc7f9f827b",
   "metadata": {},
   "source": [
    "# 2. define model & extraction point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea0b6127-f825-42c9-ac61-e4cd4bfc3e6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
      "/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = vgg16(pretrained = True)\n",
    "model.eval()\n",
    "\n",
    "return_nodes = {\n",
    "    \"features.1\": \"layer1\",\n",
    "    \"features.3\": \"layer2\",\n",
    "    \"features.6\": \"layer3\",\n",
    "    \"features.8\": \"layer4\",\n",
    "    \"features.11\": \"layer5\",\n",
    "    \"features.13\": \"layer6\",\n",
    "    \"features.15\": \"layer7\",\n",
    "    \"features.18\": \"layer8\",\n",
    "    \"features.20\": \"layer9\",\n",
    "    \"features.22\": \"layer10\",\n",
    "    \"features.25\": \"layer11\",\n",
    "    \"features.27\": \"layer12\",\n",
    "    \"features.29\": \"layer13\",\n",
    "    \"features.4\": \"layer14\",\n",
    "}\n",
    "\n",
    "extractor_model = create_feature_extractor(model, return_nodes=return_nodes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c737396-5c8f-498c-a821-9081d5e8f373",
   "metadata": {},
   "source": [
    "# 3. evaluate PCA per layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51fca877-308f-43cc-bcbc-9fafa2c49e03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1\n",
      "need at least 5 filter(s) out of 64 components to exceed threshold\n",
      "layer2\n",
      "need at least 12 filter(s) out of 64 components to exceed threshold\n",
      "layer14\n",
      "need at least 14 filter(s) out of 64 components to exceed threshold\n",
      "layer3\n",
      "need at least 25 filter(s) out of 128 components to exceed threshold\n",
      "layer4\n",
      "need at least 32 filter(s) out of 128 components to exceed threshold\n",
      "layer5\n",
      "need at least 51 filter(s) out of 256 components to exceed threshold\n",
      "layer6\n",
      "need at least 47 filter(s) out of 256 components to exceed threshold\n",
      "layer7\n",
      "need at least 32 filter(s) out of 256 components to exceed threshold\n",
      "layer8\n",
      "need at least 33 filter(s) out of 512 components to exceed threshold\n",
      "layer9\n",
      "need at least 21 filter(s) out of 512 components to exceed threshold\n",
      "layer10\n",
      "need at least 14 filter(s) out of 512 components to exceed threshold\n",
      "layer11\n",
      "need at least 9 filter(s) out of 512 components to exceed threshold\n",
      "layer12\n",
      "need at least 9 filter(s) out of 512 components to exceed threshold\n",
      "layer13\n",
      "need at least 7 filter(s) out of 512 components to exceed threshold\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1, 3, 224, 224)\n",
    "\n",
    "\n",
    "intermediate_outputs = extractor_model(x)\n",
    "\n",
    "# more important layer carries smaller pca_score\n",
    "#pca_score가 작을수록 더 중요한 레이어 입니다\n",
    "\n",
    "for k in intermediate_outputs.keys():\n",
    "    print(k)\n",
    "    feature = intermediate_outputs[k]\n",
    "    pca_score = compute_PCA(feature, 0.80, status_print = True)\n",
    "    # print(pca_score)\n",
    "    # break\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cb62af-02df-4762-ae2a-34eb1d8dab39",
   "metadata": {},
   "source": [
    "# 4. manual model(VGG) formulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16b972f1-44c8-4eca-b53f-3e20981289bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGGnet(\n",
      "  (quant): QuantStub()\n",
      "  (conv_layers): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU()\n",
      "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU()\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU()\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU()\n",
      "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU()\n",
      "    (23): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU()\n",
      "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (29): ReLU()\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (32): ReLU()\n",
      "    (33): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (36): ReLU()\n",
      "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (39): ReLU()\n",
      "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (42): ReLU()\n",
      "    (43): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fcs): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      "  (dequant): DeQuantStub()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGGnet(\n",
       "  (quant): QuantStub()\n",
       "  (conv_layers): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU()\n",
       "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU()\n",
       "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU()\n",
       "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): ReLU()\n",
       "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): ReLU()\n",
       "    (23): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (26): ReLU()\n",
       "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (29): ReLU()\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (32): ReLU()\n",
       "    (33): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (36): ReLU()\n",
       "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (39): ReLU()\n",
       "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (42): ReLU()\n",
       "    (43): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fcs): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "VGG_types = {\n",
    "    'VGG11' : [64, 'M', 128, 'M', 256, 256, 'M', 512,512, 'M',512,512,'M'],\n",
    "    'VGG13' : [64,64, 'M', 128, 128, 'M', 256, 256, 'M', 512,512, 'M', 512,512,'M'],\n",
    "    'VGG16' : [64,64, 'M', 128, 128, 'M', 256, 256,256, 'M', 512,512,512, 'M',512,512,512,'M'],\n",
    "    'VGG19' : [64,64, 'M', 128, 128, 'M', 256, 256,256,256, 'M', 512,512,512,512, 'M',512,512,512,512,'M']\n",
    "}\n",
    "\n",
    "class VGGnet(torch.nn.Module):\n",
    "    def __init__(self, model, in_channels=3, num_classes=10, init_weights=True):\n",
    "        super(VGGnet,self).__init__()\n",
    "        \n",
    "        self.quant = torch.ao.quantization.QuantStub()\t# 입력을 양자화 하는 QuantStub()\n",
    "        \n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        # create conv_layers corresponding to VGG type\n",
    "        self.conv_layers = self.create_conv_layers(VGG_types[model])\n",
    "\n",
    "        self.fcs = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        self.dequant = torch.ao.quantization.DeQuantStub() # 출력을 역양자화 하는 DeQuantStub()\n",
    "        # weight initialization\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.dequant(x)\n",
    "        x = x.view(-1, 512 * 7 * 7)\n",
    "        x = self.fcs(x)\n",
    "        return x\n",
    "\n",
    "    # defint weight initialization function\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    # define a function to create conv layer taken the key of VGG_type dict \n",
    "    def create_conv_layers(self, architecture):\n",
    "        layers = []\n",
    "        in_channels = self.in_channels # 3\n",
    "\n",
    "        for x in architecture:\n",
    "            if type(x) == int: # int means conv layer\n",
    "                out_channels = x\n",
    "\n",
    "                layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                     kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU()]\n",
    "                in_channels = x\n",
    "            elif x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))]\n",
    "\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "# define device\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print(device)\n",
    "\n",
    "# creat VGGnet object\n",
    "model = VGGnet('VGG16', in_channels=3, num_classes=10, init_weights=True)\n",
    "print(model)\n",
    "model.eval()\n",
    "# model.conv_layers[0].qconfig = torch.ao.quantization.get_default_qconfig('x86')\n",
    "# model_fp32_fused = torch.ao.quantization.fuse_modules(model, [['conv_layers']])\n",
    "# model_fp32_prepared = torch.ao.quantization.prepare(model_fp32_fused)\n",
    "# m = nn.quantized.Conv2d(512, 512, (3, 3), stride=(1, 1), padding=(1, 1))\n",
    "# model.conv_layers[27] = m\n",
    "\n",
    "# model.fcs[0].quantization\n",
    "\n",
    "# x = torch.rand(1, 3, 224, 224)\n",
    "# model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e652eded-ecd0-42d0-960a-21fc80bbe240",
   "metadata": {},
   "source": [
    "# 5. sample quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "152c8346-a04a-4c51-a850-060a88666780",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M(\n",
      "  (quant): QuantStub()\n",
      "  (conv): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (relu): ReLU()\n",
      "  (dequant): DeQuantStub()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class M(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # QuantStub converts tensors from floating point to quantized\n",
    "        self.quant = torch.ao.quantization.QuantStub()\n",
    "        self.conv = torch.nn.Conv2d(1, 1, 1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        # DeQuantStub converts tensors from quantized to floating point\n",
    "        self.dequant = torch.ao.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # manually specify where tensors will be converted from floating\n",
    "        # point to quantized in the quantized model\n",
    "        x = self.quant(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        # manually specify where tensors will be converted from quantized\n",
    "        # to floating point in the quantized model\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "# create a model instance\n",
    "model_fp32 = M()\n",
    "\n",
    "# model must be set to eval mode for static quantization logic to work\n",
    "model_fp32.eval()\n",
    "\n",
    "print(model_fp32)\n",
    "\n",
    "model_fp32.qconfig = torch.ao.quantization.get_default_qconfig('x86')\n",
    "\n",
    "\n",
    "model_fp32_fused = torch.ao.quantization.fuse_modules(model_fp32, [['conv', 'relu']])\n",
    "\n",
    "\n",
    "model_fp32_prepared = torch.ao.quantization.prepare(model_fp32_fused)\n",
    "\n",
    "input_fp32 = torch.randn(4, 1, 4, 4)\n",
    "model_fp32_prepared(input_fp32)\n",
    "\n",
    "\n",
    "model_int8 = torch.ao.quantization.convert(model_fp32_prepared)\n",
    "\n",
    "res = model_int8(input_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3514ea5a-6cb6-41fc-a2e6-694bf491461b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M(\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): MinMaxObserver(min_val=-2.575468063354492, max_val=1.9080582857131958)\n",
       "  )\n",
       "  (conv): ConvReLU2d(\n",
       "    (0): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (activation_post_process): MinMaxObserver(min_val=0.0, max_val=1.9459654092788696)\n",
       "  )\n",
       "  (relu): Identity()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e290f95-222b-474e-8c7c-f9d42905f724",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M(\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): MinMaxObserver(min_val=-2.575468063354492, max_val=1.9080582857131958)\n",
       "  )\n",
       "  (conv): ConvReLU2d(\n",
       "    (0): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (activation_post_process): MinMaxObserver(min_val=0.0, max_val=1.9459654092788696)\n",
       "  )\n",
       "  (relu): Identity()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4942912f-64a3-4549-a0cc-e47fab819fd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M(\n",
       "  (quant): QuantStub()\n",
       "  (conv): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (relu): ReLU()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
