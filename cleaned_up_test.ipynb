{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f84dc395-046e-40ed-b6b3-40774e12761d",
   "metadata": {},
   "source": [
    "# 0. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c83499ac-f736-46fd-9898-e6125ce7e9b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models.vgg import vgg16\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# paper reference: \"https://arxiv.org/abs/1906.01493\"\n",
    "# calculation reference: \"https://www.baeldung.com/cs/pca\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b793845d-d90e-450d-ba24-d30460e296e1",
   "metadata": {},
   "source": [
    "# 1. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "881e0808-7ab6-48df-91fc-e9f0dc19ce3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_PCA(feature, threshold = 0.95, status_print = False):\n",
    "\n",
    "    total_channel = feature.shape[1]\n",
    "\n",
    "    activations = (feature.data).cpu().numpy()\n",
    "    # print('shape of activations are:',activations.shape)\n",
    "    a=activations.swapaxes(1,2).swapaxes(2,3)\n",
    "    a_shape=a.shape\n",
    "    # print('reshaped ativations are of shape',a.shape)\n",
    "    # raw_input()\n",
    "\n",
    "    pca = PCA() #number of components should be equal to the number of filters\n",
    "    pca.fit(a.reshape(a_shape[0]*a_shape[1]*a_shape[2],a_shape[3]))\n",
    "    a_trans=pca.transform(a.reshape(a_shape[0]*a_shape[1]*a_shape[2],a_shape[3]))\n",
    "    # print('explained variance ratio is:',pca.explained_variance_ratio_)\n",
    "    # raw_input()\n",
    "    cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "    d = np.argmax(cumsum >= threshold)\n",
    "    \n",
    "    importance_ratio = d/total_channel\n",
    "    if status_print:\n",
    "        print('there are {} principal component(s) out of {} components, which is {:.02f}%'.format(d, total_channel, importance_ratio*100))\n",
    "    \n",
    "    return importance_ratio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f60471-6454-4a1a-b047-57cc7f9f827b",
   "metadata": {},
   "source": [
    "# 2. define model & extraction point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea0b6127-f825-42c9-ac61-e4cd4bfc3e6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
      "/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = vgg16(pretrained = True)\n",
    "model.eval()\n",
    "\n",
    "return_nodes = {\n",
    "    \"features.1\": \"layer1\",\n",
    "    \"features.3\": \"layer2\",\n",
    "    \"features.6\": \"layer3\",\n",
    "    \"features.8\": \"layer4\",\n",
    "    \"features.11\": \"layer5\",\n",
    "    \"features.13\": \"layer6\",\n",
    "    \"features.15\": \"layer7\",\n",
    "    \"features.18\": \"layer8\",\n",
    "    \"features.20\": \"layer9\",\n",
    "    \"features.22\": \"layer10\",\n",
    "    \"features.25\": \"layer11\",\n",
    "    \"features.27\": \"layer12\",\n",
    "    \"features.29\": \"layer13\",\n",
    "    \"features.4\": \"layer14\",\n",
    "}\n",
    "\n",
    "extractor_model = create_feature_extractor(model, return_nodes=return_nodes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c737396-5c8f-498c-a821-9081d5e8f373",
   "metadata": {},
   "source": [
    "# 3. evaluate PCA per layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51fca877-308f-43cc-bcbc-9fafa2c49e03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 14 principal component(s) out of 64 components, which is 21.88%\n",
      "0.21875\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1, 3, 224, 224)\n",
    "\n",
    "\n",
    "intermediate_outputs = extractor_model(x)\n",
    "\n",
    "# print(type(intermediate_outputs['layer11'].shape[1]))\n",
    "# intermediate_outputs.keys()\n",
    "\n",
    "for k in intermediate_outputs.keys():\n",
    "    feature = intermediate_outputs[k]\n",
    "    pca_score = compute_PCA(feature, status_print = True)\n",
    "    print(pca_score)\n",
    "    break\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c27da180-14ce-4753-906d-4fe3c340c01d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.features[0][0].qconfig = torch.ao.quantization.get_default_qat_qconfig('x86')\n",
    "import torch.nn as nn\n",
    "m = nn.quantized.Conv2d(512, 512, (3, 3), stride=(1, 1), padding=(1, 1))\n",
    "model.features[19] = m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5310906-0103-483f-a04e-33d2ab426d87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.cpu()\n",
    "x = torch.rand(1, 3, 224, 224)\n",
    "model(x.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eb26acf-0c89-4ad1-b71d-780c9a850682",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'VGG' object has no attribute 'qconfig'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_640266/3414250890.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1206\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m-> 1208\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m   1209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'VGG' object has no attribute 'qconfig'"
     ]
    }
   ],
   "source": [
    "model.qconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57458d5b-0e22-4b6e-a641-846ddcb9eb69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c163e0-1b5f-404e-bfd1-726ee43e2ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16b972f1-44c8-4eca-b53f-3e20981289bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGGnet(\n",
      "  (quant): QuantStub()\n",
      "  (conv_layers): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU()\n",
      "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU()\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU()\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU()\n",
      "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU()\n",
      "    (23): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU()\n",
      "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (29): ReLU()\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (32): ReLU()\n",
      "    (33): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (36): ReLU()\n",
      "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (39): ReLU()\n",
      "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (42): ReLU()\n",
      "    (43): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fcs): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      "  (dequant): DeQuantStub()\n",
      ")\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "did not find fuser method for: (<class 'torch.nn.modules.container.Sequential'>,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_658344/2208377513.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mao\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_qconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x86'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0mmodel_fp32_fused\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mao\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuse_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'conv_layers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0mmodel_fp32_prepared\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mao\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fp32_fused\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;31m# m = nn.quantized.Conv2d(512, 512, (3, 3), stride=(1, 1), padding=(1, 1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/ao/quantization/fuse_modules.py\u001b[0m in \u001b[0;36mfuse_modules\u001b[0;34m(model, modules_to_fuse, inplace, fuser_func, fuse_custom_config_dict)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mfuser_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfuse_known_modules\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         fuse_custom_config_dict=None)\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfuse_modules_qat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodules_to_fuse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuser_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfuse_known_modules\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuse_custom_config_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/ao/quantization/fuse_modules.py\u001b[0m in \u001b[0;36m_fuse_modules\u001b[0;34m(model, modules_to_fuse, is_qat, inplace, fuser_func, fuse_custom_config_dict)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# Handle case of modules_to_fuse being a list of lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule_list\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules_to_fuse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0m_fuse_modules_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_qat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuser_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuse_custom_config_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/ao/quantization/fuse_modules.py\u001b[0m in \u001b[0;36m_fuse_modules_helper\u001b[0;34m(model, modules_to_fuse, is_qat, fuser_func, fuse_custom_config_dict)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;31m# Fuse list of modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0mnew_mod_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuser_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_qat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madditional_fuser_method_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;31m# Replace original module list with fused module list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/ao/quantization/fuse_modules.py\u001b[0m in \u001b[0;36mfuse_known_modules\u001b[0;34m(mod_list, is_qat, additional_fuser_method_mapping)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \"\"\"\n\u001b[1;32m     45\u001b[0m     \u001b[0mtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_before_parametrizations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmod_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mfuser_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_fuser_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madditional_fuser_method_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfuser_method\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot fuse modules: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/ao/quantization/fuser_method_mappings.py\u001b[0m in \u001b[0;36mget_fuser_method\u001b[0;34m(op_list, additional_fuser_method_mapping)\u001b[0m\n\u001b[1;32m    178\u001b[0m                                      additional_fuser_method_mapping)\n\u001b[1;32m    179\u001b[0m     \u001b[0mfuser_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_mappings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mfuser_method\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"did not find fuser method for: {} \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfuser_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: did not find fuser method for: (<class 'torch.nn.modules.container.Sequential'>,) "
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "VGG_types = {\n",
    "    'VGG11' : [64, 'M', 128, 'M', 256, 256, 'M', 512,512, 'M',512,512,'M'],\n",
    "    'VGG13' : [64,64, 'M', 128, 128, 'M', 256, 256, 'M', 512,512, 'M', 512,512,'M'],\n",
    "    'VGG16' : [64,64, 'M', 128, 128, 'M', 256, 256,256, 'M', 512,512,512, 'M',512,512,512,'M'],\n",
    "    'VGG19' : [64,64, 'M', 128, 128, 'M', 256, 256,256,256, 'M', 512,512,512,512, 'M',512,512,512,512,'M']\n",
    "}\n",
    "\n",
    "class VGGnet(torch.nn.Module):\n",
    "    def __init__(self, model, in_channels=3, num_classes=10, init_weights=True):\n",
    "        super(VGGnet,self).__init__()\n",
    "        \n",
    "        self.quant = torch.ao.quantization.QuantStub()\t# 입력을 양자화 하는 QuantStub()\n",
    "        \n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        # create conv_layers corresponding to VGG type\n",
    "        self.conv_layers = self.create_conv_layers(VGG_types[model])\n",
    "\n",
    "        self.fcs = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        self.dequant = torch.ao.quantization.DeQuantStub() # 출력을 역양자화 하는 DeQuantStub()\n",
    "        # weight initialization\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.dequant(x)\n",
    "        x = x.view(-1, 512 * 7 * 7)\n",
    "        x = self.fcs(x)\n",
    "        return x\n",
    "\n",
    "    # defint weight initialization function\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    # define a function to create conv layer taken the key of VGG_type dict \n",
    "    def create_conv_layers(self, architecture):\n",
    "        layers = []\n",
    "        in_channels = self.in_channels # 3\n",
    "\n",
    "        for x in architecture:\n",
    "            if type(x) == int: # int means conv layer\n",
    "                out_channels = x\n",
    "\n",
    "                layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                     kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU()]\n",
    "                in_channels = x\n",
    "            elif x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))]\n",
    "\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "# define device\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print(device)\n",
    "\n",
    "# creat VGGnet object\n",
    "model = VGGnet('VGG16', in_channels=3, num_classes=10, init_weights=True)\n",
    "print(model)\n",
    "model.eval()\n",
    "model.conv_layers[0].qconfig = torch.ao.quantization.get_default_qconfig('x86')\n",
    "model_fp32_fused = torch.ao.quantization.fuse_modules(model, [['conv_layers']])\n",
    "model_fp32_prepared = torch.ao.quantization.prepare(model_fp32_fused)\n",
    "# m = nn.quantized.Conv2d(512, 512, (3, 3), stride=(1, 1), padding=(1, 1))\n",
    "# model.conv_layers[27] = m\n",
    "\n",
    "# model.fcs[0].quantization\n",
    "\n",
    "# x = torch.rand(1, 3, 224, 224)\n",
    "# model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc40f1ba-e536-4185-9d26-80c339abf829",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = VGGnet('VGG16', in_channels=3, num_classes=10, init_weights=True).to(\"cpu\")\n",
    "print(model)\n",
    "\n",
    "# m = nn.quantized.Conv2d(512, 512, (3, 3), stride=(1, 1), padding=(1, 1))\n",
    "# model.conv_layers[27] = m\n",
    "\n",
    "backend = \"qnnpack\"\n",
    "model.qconfig = torch.quantization.get_default_qconfig(backend)\n",
    "torch.backends.quantized.engine = backend\n",
    "model_static_quantized = torch.quantization.prepare(model, inplace = False)\n",
    "# model_static_quantized = torch.quantization.convert(model_static_quantized, inplace = False)\n",
    "model_static_quantized\n",
    "\n",
    "# m = nn.quantized.Conv2d(512, 512, (3, 3), stride=(1, 1), padding=(1, 1))\n",
    "# model_static_quantized.conv_layers[27] = m\n",
    "\n",
    "x = torch.rand(1, 3, 224, 224)\n",
    "model_static_quantized(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e46f14e-c426-4340-9752-3a6e150e1ba1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M(\n",
      "  (quant): QuantStub()\n",
      "  (conv): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (relu): ReLU()\n",
      "  (dequant): DeQuantStub()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "class M(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # QuantStub converts tensors from floating point to quantized\n",
    "        self.quant = torch.ao.quantization.QuantStub()\n",
    "        self.conv = torch.nn.Conv2d(1, 1, 1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        # DeQuantStub converts tensors from quantized to floating point\n",
    "        self.dequant = torch.ao.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # manually specify where tensors will be converted from floating\n",
    "        # point to quantized in the quantized model\n",
    "        x = self.quant(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        # manually specify where tensors will be converted from quantized\n",
    "        # to floating point in the quantized model\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "# create a model instance\n",
    "model_fp32 = M()\n",
    "\n",
    "# model must be set to eval mode for static quantization logic to work\n",
    "model_fp32.eval()\n",
    "print(model_fp32)\n",
    "\n",
    "model_fp32.qconfig = torch.ao.quantization.get_default_qconfig('x86')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "152c8346-a04a-4c51-a850-060a88666780",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M(\n",
      "  (quant): QuantStub()\n",
      "  (conv): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (relu): ReLU()\n",
      "  (dequant): DeQuantStub()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class M(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # QuantStub converts tensors from floating point to quantized\n",
    "        self.quant = torch.ao.quantization.QuantStub()\n",
    "        self.conv = torch.nn.Conv2d(1, 1, 1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        # DeQuantStub converts tensors from quantized to floating point\n",
    "        self.dequant = torch.ao.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # manually specify where tensors will be converted from floating\n",
    "        # point to quantized in the quantized model\n",
    "        x = self.quant(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        # manually specify where tensors will be converted from quantized\n",
    "        # to floating point in the quantized model\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "# create a model instance\n",
    "model_fp32 = M()\n",
    "\n",
    "# model must be set to eval mode for static quantization logic to work\n",
    "model_fp32.eval()\n",
    "\n",
    "print(model_fp32)\n",
    "\n",
    "model_fp32.qconfig = torch.ao.quantization.get_default_qconfig('x86')\n",
    "\n",
    "\n",
    "model_fp32_fused = torch.ao.quantization.fuse_modules(model_fp32, [['conv', 'relu']])\n",
    "\n",
    "\n",
    "model_fp32_prepared = torch.ao.quantization.prepare(model_fp32_fused)\n",
    "\n",
    "input_fp32 = torch.randn(4, 1, 4, 4)\n",
    "model_fp32_prepared(input_fp32)\n",
    "\n",
    "\n",
    "model_int8 = torch.ao.quantization.convert(model_fp32_prepared)\n",
    "\n",
    "res = model_int8(input_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3514ea5a-6cb6-41fc-a2e6-694bf491461b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M(\n",
       "  (quant): QuantStub()\n",
       "  (conv): ConvReLU2d(\n",
       "    (0): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (relu): Identity()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e290f95-222b-474e-8c7c-f9d42905f724",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M(\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): MinMaxObserver(min_val=-2.6162924766540527, max_val=2.1399834156036377)\n",
       "  )\n",
       "  (conv): ConvReLU2d(\n",
       "    (0): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (activation_post_process): MinMaxObserver(min_val=0.0, max_val=0.7080661058425903)\n",
       "  )\n",
       "  (relu): Identity()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4942912f-64a3-4549-a0cc-e47fab819fd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M(\n",
       "  (quant): QuantStub()\n",
       "  (conv): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (relu): ReLU()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
